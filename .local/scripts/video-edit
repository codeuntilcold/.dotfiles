#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "openai-whisper",
#     "ffmpeg-python",
# ]
# ///

"""
Video Tutorial Processor CLI
Processes videos by transcribing and speeding up silent gaps.
"""

import argparse
import hashlib
import json
import logging
import os
import shutil
import sys
import tempfile
from dataclasses import dataclass
from typing import Dict, List

import ffmpeg
import whisper


@dataclass
class Segment:
    start: float
    end: float
    text: str
    speed: float
    reason: str


class VideoProcessor:
    def __init__(self, config: Dict):
        self.config = config
        self.cache_dir = config.get("cache_dir", ".video_cache")
        os.makedirs(self.cache_dir, exist_ok=True)

        # Setup logging
        log_level = getattr(logging, config.get("log_level", "INFO").upper())
        logging.basicConfig(
            level=log_level,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler("video_processor.log")
                if config.get("log_file")
                else logging.NullHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

        # Lazy load Whisper
        self.whisper_model = None
        self.whisper_model_size = config.get("whisper_model", "base")

    def _get_cache_path(self, video_path: str, suffix: str) -> str:
        """Generate cache file path"""
        basename = os.path.basename(video_path)
        name, _ = os.path.splitext(basename)
        stat = os.stat(video_path)
        cache_key = f"{name}_{stat.st_size}_{int(stat.st_mtime)}"
        return os.path.join(self.cache_dir, f"{cache_key}_{suffix}")

    def extract_transcript(self, video_path: str) -> Dict:
        """Extract transcript using Whisper with caching"""
        cache_path = self._get_cache_path(video_path, "transcript.json")

        # Try loading from cache
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r") as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"Failed to load cached transcript: {e}")

        # Load Whisper model if needed
        if not self.whisper_model:
            self.logger.info(f"Loading Whisper model: {self.whisper_model_size}")
            self.whisper_model = whisper.load_model(self.whisper_model_size)

        # Transcribe
        self.logger.info(f"Transcribing video: {video_path}")
        result = self.whisper_model.transcribe(video_path, verbose=False)

        segments = [
            {
                "start": seg["start"],
                "end": seg["end"],
                "text": seg["text"].strip(),
            }
            for seg in result["segments"]
        ]

        transcript_data = {
            "segments": segments,
            "language": result.get("language", "unknown"),
        }

        # Cache the result
        try:
            with open(cache_path, "w") as f:
                json.dump(transcript_data, f, indent=2)
        except Exception as e:
            self.logger.warning(f"Failed to cache transcript: {e}")

        self.logger.info(f"Transcription complete. Found {len(segments)} segments.")
        return transcript_data

    def create_segments(self, video_path: str, transcript_data: Dict) -> List[Segment]:
        """Create segments with speed assignments"""
        segments = []
        fast_speed = self.config.get("fast_speed", 5.0)

        # Get video duration
        try:
            probe = ffmpeg.probe(video_path)
            video_duration = float(probe["format"]["duration"])
        except Exception as e:
            self.logger.error(f"Failed to get video duration: {e}")
            return segments

        # Convert transcript segments to 1x speed
        for seg in transcript_data["segments"]:
            segments.append(
                Segment(
                    start=seg["start"],
                    end=seg["end"],
                    text=seg["text"],
                    speed=1.0,
                    reason="Audio content",
                )
            )

        # Sort by start time
        segments.sort(key=lambda s: s.start)

        # Fill gaps with fast segments
        filled_segments = []
        prev_end = 0

        for segment in segments:
            # Add gap before segment if exists
            if segment.start > prev_end + 0.5:
                filled_segments.append(
                    Segment(
                        start=prev_end,
                        end=segment.start,
                        text="[Silent gap]",
                        speed=fast_speed,
                        reason="No audio",
                    )
                )

            filled_segments.append(segment)
            prev_end = segment.end

        # Add final gap if needed
        if prev_end < video_duration - 0.5:
            filled_segments.append(
                Segment(
                    start=prev_end,
                    end=video_duration,
                    text="[Silent gap]",
                    speed=fast_speed,
                    reason="No audio",
                )
            )

        # Merge consecutive segments with same speed
        merged = []
        current = filled_segments[0]

        for next_seg in filled_segments[1:]:
            if (
                current.speed == next_seg.speed
                and abs(current.end - next_seg.start) < 0.1
            ):
                # Merge segments
                current = Segment(
                    start=current.start,
                    end=next_seg.end,
                    text=f"{current.text} + {next_seg.text}",
                    speed=current.speed,
                    reason=f"Merged: {current.reason}",
                )
            else:
                merged.append(current)
                current = next_seg

        merged.append(current)

        gap_count = len(filled_segments) - len(transcript_data["segments"])
        merge_count = len(filled_segments) - len(merged)

        self.logger.info(
            f"Created {len(merged)} segments ({gap_count} gaps added, {merge_count} merged)"
        )
        return merged

    def process_segments(
        self, video_path: str, segments: List[Segment], output_path: str
    ):
        """Process video segments with caching"""
        self.logger.info("Processing video segments...")

        with tempfile.TemporaryDirectory() as temp_dir:
            segment_files = []

            for i, segment in enumerate(segments):
                duration = segment.end - segment.start

                # Generate cache key
                segment_data = (
                    f"{segment.start}_{segment.end}_{segment.speed}_{duration}"
                )
                segment_hash = hashlib.md5(segment_data.encode()).hexdigest()[:12]
                cached_path = self._get_cache_path(
                    video_path, f"segment_{segment_hash}.mp4"
                )

                segment_file = os.path.join(temp_dir, f"segment_{i:03d}.mp4")

                # Use cached segment if available
                if os.path.exists(cached_path):
                    self.logger.debug(f"Using cached segment {i + 1}")
                    shutil.copy2(cached_path, segment_file)
                    segment_files.append(segment_file)
                    continue

                # Process segment
                try:
                    input_stream = ffmpeg.input(
                        video_path, ss=segment.start, t=duration
                    )

                    if segment.speed > 1.0:
                        # Fast segment with silent audio
                        video_stream = input_stream.video.filter(
                            "setpts", f"PTS/{segment.speed}"
                        )
                        sped_duration = duration / segment.speed
                        silent_audio = ffmpeg.input(
                            f"anullsrc=duration={sped_duration}:sample_rate=44100:channel_layout=stereo",
                            f="lavfi",
                        )
                        output_stream = ffmpeg.output(
                            video_stream,
                            silent_audio,
                            segment_file,
                            vcodec="libx264",
                            acodec="aac",
                            avoid_negative_ts="make_zero",
                        )
                    else:
                        # Normal speed with original audio
                        output_stream = ffmpeg.output(
                            input_stream,
                            segment_file,
                            vcodec="libx264",
                            acodec="aac",
                            avoid_negative_ts="make_zero",
                        )

                    output_stream.overwrite_output().run(quiet=True)

                    # Cache the processed segment
                    shutil.copy2(segment_file, cached_path)
                    segment_files.append(segment_file)

                except ffmpeg.Error as e:
                    self.logger.error(f"Failed to process segment {i + 1}: {e}")
                    continue

            # Concatenate segments
            if not segment_files:
                raise ValueError("No segments were processed!")

            concat_file = os.path.join(temp_dir, "concat_list.txt")
            with open(concat_file, "w") as f:
                for segment_file in segment_files:
                    f.write(f"file '{segment_file}'\n")

            self.logger.info(f"Concatenating {len(segment_files)} segments...")
            (
                ffmpeg.input(concat_file, format="concat", safe=0)
                .output(
                    output_path,
                    vcodec="libx264",
                    acodec="aac",
                    avoid_negative_ts="make_zero",
                )
                .overwrite_output()
                .run(quiet=True)
            )

            self.logger.info(f"Video processing complete: {output_path}")

    def process_video(self, video_path: str, output_path: str) -> Dict:
        """Main processing pipeline"""
        self.logger.info(f"Processing video: {video_path}")

        # Extract transcript
        transcript_data = self.extract_transcript(video_path)

        # Create segments
        segments = self.create_segments(video_path, transcript_data)

        # Process video
        self.process_segments(video_path, segments, output_path)

        # Calculate stats
        normal_segments = [s for s in segments if s.speed == 1.0]
        fast_segments = [s for s in segments if s.speed != 1.0]
        total_duration = sum(s.end - s.start for s in segments)
        effective_duration = sum((s.end - s.start) / s.speed for s in segments)

        return {
            "input_file": video_path,
            "output_file": output_path,
            "original_duration": total_duration,
            "effective_duration": effective_duration,
            "time_saved": total_duration - effective_duration,
            "normal_speed_segments": len(normal_segments),
            "fast_speed_segments": len(fast_segments),
        }


def main():
    parser = argparse.ArgumentParser(
        description="Process videos by speeding up silent gaps"
    )
    parser.add_argument("input", help="Input video file path")
    parser.add_argument("-o", "--output", required=True, help="Output video file path")
    parser.add_argument(
        "--whisper-model",
        default="base",
        choices=["tiny", "base", "small", "medium", "large"],
        help="Whisper model size (default: base)",
    )
    parser.add_argument(
        "--fast-speed",
        type=float,
        default=5.0,
        help="Speed multiplier for silent segments (default: 5.0)",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Logging level (default: INFO)",
    )
    parser.add_argument(
        "--log-file", action="store_true", help="Enable logging to file"
    )
    parser.add_argument(
        "--cache-dir",
        default=".video_cache",
        help="Directory for cached files (default: .video_cache)",
    )

    args = parser.parse_args()

    # Validate input
    if not os.path.exists(args.input):
        print(f"Error: Input file '{args.input}' not found")
        sys.exit(1)

    # Create output directory
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Configuration
    config = {
        "whisper_model": args.whisper_model,
        "fast_speed": args.fast_speed,
        "log_level": args.log_level,
        "log_file": args.log_file,
        "cache_dir": args.cache_dir,
    }

    try:
        processor = VideoProcessor(config)
        result = processor.process_video(args.input, args.output)

        # Print results
        print("\n" + "=" * 50)
        print("VIDEO PROCESSING COMPLETE")
        print("=" * 50)
        print(f"Input:     {result['input_file']}")
        print(f"Output:    {result['output_file']}")
        print(
            f"Duration:  {result['original_duration']:.1f}s â†’ {result['effective_duration']:.1f}s"
        )
        print(
            f"Segments:  {result['normal_speed_segments']} normal, {result['fast_speed_segments']} fast"
        )
        print(
            f"Time saved: {result['time_saved']:.1f}s ({(result['time_saved'] / result['original_duration']) * 100:.1f}%)"
        )

    except KeyboardInterrupt:
        print("\nProcessing interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
